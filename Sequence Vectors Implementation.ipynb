{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "imdb_data_path = os.path.join('aclImdb')\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "    for fname in sorted(os.listdir(train_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(train_path, fname)) as f:\n",
    "                train_texts.append(f.read())\n",
    "            train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "    for fname in sorted(os.listdir(test_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(test_path, fname)) as f:\n",
    "                test_texts.append(f.read())\n",
    "            test_labels.append(0 if category == 'neg' else 1)\n",
    "            \n",
    "random.seed(123)\n",
    "random.shuffle(train_texts)\n",
    "random.seed(123)\n",
    "random.shuffle(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary with training texts.\n",
    "tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Vectorize training and validation texts.\n",
    "x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "x_val = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Get max sequence length.\n",
    "max_length = len(max(x_train, key=len))\n",
    "if max_length > MAX_SEQUENCE_LENGTH:\n",
    "    max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "# Fix sequence length to max value. Sequences shorter than the length are\n",
    "# padded in the beginning and sequences longer are truncated\n",
    "# at the beginning.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, indexes = sequence_vectorize(train_texts, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
