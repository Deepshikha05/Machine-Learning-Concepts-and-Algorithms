{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Sequence Vectors are better?\n",
    "The answer to above question is that Sequence Vectors take account of order of words and word order is critical to the text's meaning.\n",
    "For example, the sentences, “I used to hate my commute. My new bike changed that completely” can be understood only when read in order. Models such as CNNs/RNNs can infer meaning from the order of words in a sample. \n",
    "For these models, we represent the text as a sequence of tokens, preserving order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "- Text can be represented as either a sequence of characters, or a sequence of words. \n",
    "- It is found that using word-level representation provides better performance than character tokens. \n",
    "- Using character tokens makes sense only if texts have lots of typos, which isn’t normally the case.\n",
    "\n",
    "#### Vectorization\n",
    "- Once text samples have been converted into sequence of words, it then needs to be converted into sequence of numerical vectors.\n",
    "- Below shows the indexes assigned to the unigrams generated for two texts, and then the sequence of token indexes to which the first text is converted.\n",
    "- Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
    "- Index assigned for every token: {'clock': 5, 'ran': 3, 'up': 4, 'down': 6, 'the': 1, 'mouse': 2}.\n",
    "- NOTE: 'the' occurs most frequently, so the index value of 1 is assigned to it.\n",
    "- Some libraries reserve index 0 for unknown tokens, as is the case here.\n",
    "- Sequence of token indexes: 'The mouse ran up the clock' = [1, 2, 3, 4, 1, 5]\n",
    "\n",
    "#### One-Hot Encoding\n",
    "- Sequences are represented using word vectors in n- dimensional space where n = size of vocabulary.\n",
    "- This representation works great when we are tokenizing as characters, and the vocabulary is therefore small. \n",
    "- When we are tokenizing as words, the vocabulary will usually have tens of thousands of tokens, making the one-hot vectors very sparse and inefficient. \n",
    "- Example:\n",
    "            'The mouse ran up the clock' = [\n",
    "              [0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0]\n",
    "            ]\n",
    "            \n",
    "#### Word Embeddings\n",
    "- Words have meaning(s) associated with them. As a result, we can represent word tokens in a dense vector space (~few hundred real numbers), where the location and distance between words indicates how similar they are semantically.\n",
    "- Sequence models often have such an embedding layer as their first layer. This layer learns to turn word index sequences into word embedding vectors during the training process, such that each word index gets mapped to a dense vector of real values representing that word’s location in semantic space.\n",
    "\n",
    "\n",
    "#### Feature Selection\n",
    "- Not all words in our data contribute to label predictions. We can optimize our learning process by discarding rare or irrelevant words from our vocabulary. \n",
    "\n",
    "#### Summarizing \n",
    "- Tokenize text into words\n",
    "- Creates a vocabulary using 20k tokens\n",
    "- Converts sequence into sequence vectors\n",
    "- Pads the sequence to a fixed sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Vectorization\n",
    "- The way we convert sample text data into sequence vectors, similar operation can be applied to the labels as well.\n",
    "- We can convert labels into values in range [0, no_of_classes -1]\n",
    "- For example, if there are 3 classes we can just use values 0, 1 and 2 to represent them. \n",
    "- Internally, the network will use one-hot vectors to represent these values (to avoid inferring an incorrect relationship between labels). This representation depends on the loss function and the last- layer activation function we use in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
