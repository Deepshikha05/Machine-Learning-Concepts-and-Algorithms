{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "- A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. \n",
    "- The topmost node in a decision tree is known as the root node. \n",
    "- It learns to partition on the basis of the attribute value.\n",
    "- It partitions the tree in recursively manner call __recursive partitioning__. \n",
    "- This flowchart-like structure helps you in decision making. \n",
    "\n",
    "- Decision Tree is a white box type of ML algorithm. \n",
    "- It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. \n",
    "- Its training time is faster compared to the neural network algorithm. \n",
    "- The time complexity of decision trees is a function of the number of records and number of attributes in the given data. \n",
    "- The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. \n",
    "- Decision trees can handle high dimensional data with good accuracy.\n",
    "\n",
    "\n",
    "The basic idea behind any decision tree algorithm is as follows:\n",
    "\n",
    "1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n",
    "2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
    "3. Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
    "    1. All the tuples belong to the same attribute value.\n",
    "    2. There are no more remaining attributes.\n",
    "    3. There are no more instances.\n",
    "    \n",
    "    \n",
    "<img src=\"images/decision_tress2.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Selection Measures\n",
    "\n",
    "Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute. In the case of a continuous-valued attribute, split points for branches also need to define. \n",
    "Types of Selection Measures:\n",
    "1. Information Gain\n",
    "2. Gain Ratio\n",
    "3. Gini\n",
    "4. Chi Square\n",
    "5. Reduction in Variance\n",
    "\n",
    "__Entropy__: Measure of disorder or measure of impurity. \n",
    "<img src=\"images/entropy.png\">\n",
    "\n",
    "Where ‘Pi’ is simply the frequentist probability of an element/class ‘i’ in our data.\n",
    "\n",
    "Example, let’s say we only have two classes , a positive class and a negative class. Therefore ‘i’ here could be either + or (-). So if we had a total of 100 data points in our dataset with 30 belonging to the positive class and 70 belonging to the negative class then ‘P+’ would be 3/10 and ‘P-’ would be 7/10. Pretty straightforward.\n",
    "If I was to calculate the entropy of my classes in this example using the formula above. Here’s what I would get.\n",
    "\n",
    "<img src=\"images/entropy1.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "0.88 is considered a high entropy , a high level of disorder ( meaning low level of purity). Entropy is measured between 0 and 1.(Depending on the number of classes in your dataset, entropy can be greater than 1 but it means the same thing , a very high level of disorder.)\n",
    "\n",
    "- Does it matter why entropy is measured using log base 2 or why entropy is measured between 0 and 1 and not some other range? No. It’s just a metric. It’s not important to know how it came to be. It’s important to know how to read it and what it tells us, which we just did above. Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "\n",
    "\n",
    "\n",
    "__1. Information Gain__:Shannon invented the concept of entropy, which measures the impurity of the input set. In physics and mathematics, entropy referred as the randomness or the impurity in the system. In information theory, it refers to the impurity in a group of examples. Information gain is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n",
    "\n",
    "<img src=\"images/info_gain4.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "The attribute A with the highest information gain, Gain(A), is chosen as the splitting attribute at node N().\n",
    "\n",
    "\n",
    "__2. Gain Ration__:Information gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n",
    "\n",
    "C4.5, an improvement of ID3, uses an extension to information gain known as the gain ratio. Gain ratio handles the issue of bias by normalizing the information gain using Split Info. Java implementation of the C4.5 algorithm is known as J48, which is available in WEKA data mining tool.\n",
    "\n",
    "<img src=\"images/gain_ratio1.png\">\n",
    "Where,\n",
    "\n",
    "- |Dj|/|D| acts as the weight of the jth partition.\n",
    "- v is the number of discrete values in attribute A.\n",
    "The gain ratio can be defined as:\n",
    "<img src=\"images/gain_ratio2.png\">\n",
    "\n",
    "The attribute with the highest gain ratio is chosen as the splitting attribute.\n",
    "\n",
    "__3. Gini__:Gini  says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "\n",
    "    - It works with categorical target variable “Success” or “Failure”.\n",
    "    - It performs only Binary splits\n",
    "    - Higher the value of Gini higher the homogeneity.\n",
    "    - CART (Classification and Regression Tree) uses Gini method to create binary splits.\n",
    "    \n",
    "    Steps to Calculate Gini for a split\n",
    "    - Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n",
    "    - Calculate Gini for split using weighted Gini score of each node of that split\n",
    "    \n",
    " Gini Impurity = 1-Gini\n",
    "\n",
    "__4.Chi-Square__: It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.\n",
    "\n",
    "    - It works with categorical target variable “Success” or “Failure”.\n",
    "    - It can perform two or more splits.\n",
    "    - Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n",
    "    - Chi-Square of each node is calculated using formula,\n",
    "    - Chi-square = ((Actual – Expected)^2 / Expected)^1/2\n",
    "    - It generates tree called CHAID (Chi-square Automatic Interaction Detector)\n",
    "    \n",
    "    Steps to Calculate Chi-square for a split:\n",
    "    - Calculate Chi-square for individual node by calculating the deviation for Success and Failure both\n",
    "    - Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split\n",
    "    \n",
    "__5.Reduction in Variance__:Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population:\n",
    "<img src=\"images/variance.png\">\n",
    "\n",
    "    Steps to calculate Variance:\n",
    "    - Calculate variance for each node.\n",
    "    - Calculate variance for each split as weighted average of each node variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key parameters of tree modeling \n",
    "\n",
    "Overfitting is one of the key challenges faced while modeling decision trees. If there is no limit set of a decision tree, it will give 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\n",
    "\n",
    "1. Setting constraints on tree size\n",
    "2. Tree pruning\n",
    "\n",
    "__1. Setting Constraints on Tree Size__\n",
    "This can be done by using various parameters which are used to define a tree. \n",
    "\n",
    "    1. Minimum samples for a node split:\n",
    "            - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "            - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "            - Too high values can lead to under-fitting hence, it should be tuned using cross-validation.\n",
    "    2. Minimum samples for a terminal node (leaf): \n",
    "        - Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    "        - Used to control over-fitting similar to min_samples_split.\n",
    "        - Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n",
    "    3. Maximum depth of tree:\n",
    "        - It is defined by maximum depth of a tree.\n",
    "        - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample. Should be tuned using cross-validation.\n",
    "    4. Maximum number of terminal nodes:\n",
    "        - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    5. Maximum features to consider for split:\n",
    "        - The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "        - As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "        - Higher values can lead to over-fitting but depends on case to case.\n",
    "        \n",
    "__2. Tree Pruning__: \n",
    "    1. We first make the decision tree to a large depth.\n",
    "    2. Then we start at the bottom and start removing leaves which are giving us negative returns when compared from the top.\n",
    "    3. Suppose a split is giving us a gain of say -10 (loss of 10) and then the next split on that gives us a gain of 20. A simple decision tree will stop at step 1 but in pruning, we will see that the overall gain is +10 and keep both leaves.\n",
    "    \n",
    "Sklearn’s decision tree classifier does not currently support pruning. Advanced packages like xgboost have adopted tree pruning in their implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
