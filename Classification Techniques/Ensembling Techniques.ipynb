{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an ensemble method:\n",
    "The idea here is to train multiple models, each with the objective to predict or classify a set of results.\n",
    "\n",
    "Most of the errors from a model’s learning are from three main factors: variance, noise, and bias. \n",
    "\n",
    "By using ensemble methods, we’re able to increase the stability of the final model and reduce the errors mentioned previously. \n",
    "\n",
    "By combining many models, we’re able to reduce the variance, even when they are individually not great, as we won’t suffer from random errors from a single source.\n",
    "\n",
    "The main principle behind ensemble modelling is to group weak learners together to form one strong learner.\n",
    "\n",
    "There are three main ensembling techniques:\n",
    "1. Bagging -> to decrease the model’s variance;\n",
    "2. Boosting -> to decreasing the model’s bias, and;\n",
    "3. Stacking -> to increasing the predictive force of the classifier.\n",
    "\n",
    "__1. Bagging__:It is shorthand for the combination of bootstrapping and aggregating. Bootstrapping is a method to help decrease the variance of the classifier and reduce overfitting, by resampling data from the training set with the same cardinality as the original set. The model created should be less overfitted than a single individual model.\n",
    "\n",
    "A high variance for a model is not good, suggesting its performance is sensitive to the training data provided. So, even if more the training data is provided, the model may still perform poorly. And, may not even reduce the variance of our model.\n",
    "\n",
    "Bagging is an effective method when there is limited data, and by using samples you’re able to get an estimate by aggregating the scores over many samples.\n",
    "\n",
    "The simplest approach with bagging is to use a couple of small subsamples and bag them, if the ensemble accuracy is much higher than the base models, it’s working; if not, use larger subsamples.\n",
    "\n",
    "In bagging there is a tradeoff between base model accuracy and the gain you get through bagging. The aggregation from bagging may improve the ensemble greatly when you have an unstable model, yet when your base models are more stable — been trained on larger subsamples with higher accuracy — improvements from bagging reduces.\n",
    "\n",
    "Once the bagging is done, and all the models have been created on (mostly) different data, a weighted average is then used to determine the final score.\n",
    "\n",
    "__2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
